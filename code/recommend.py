# -*- coding: utf-8 -*-
"""recommentition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lv5xI6v_4K_RwJSt-4z7B6CTwrEMNOUR

## 导入包以及设置全局变量
<hr>
"""

# connect with google drive
from google.colab import drive
drive.mount('/content/drive')

# 全局变量
# 设置随机数种子
SEED = 1
# 文件位置
FILE_PATH = ''
# 当前的路径
import os
os.chdir("/content/drive/My Drive/recommention")

"""## 数据预处理
<hr>
"""

import pickle
# 加载train_data 数据类型dict嵌套
# {use_ed:{item_id:score}}
def load_train_data(filepath, output_pickle=False, input_pickle=False):
    print('begin load train data')
    # 如果选择导入pickle格式的train数据集
    if input_pickle is True:
        train = pickle.load(open(FILE_PATH + 'train.pickle', 'rb'))
    else:  # 选择导入 txt格式的训练集
        with open(filepath, 'r') as f:
            train = {}
            while True:
                line = f.readline()
                if not line or line == '\n':
                    break

                id, item_num = line.split('|')
                id = int(id)
                item_num = int(item_num)
                item = {}
                # 遍历之后的内容
                for i in range(item_num):
                    line = f.readline()
                    item_id, score = line.split("  ")[:2]
                    # 数据类型转化
                    score = int(score)
                    item_id = int(item_id)
                    # 放入字典中
                    item[item_id] = score
                # 字典嵌套
                print(item)
                train[id] = item

    # print(train)

    # 使用dump()将数据序列化到文件中
    if output_pickle is True:
        with open(FILE_PATH + 'train.pickle', 'wb') as handle:
            pickle.dump(train, handle)
    print('load train data finish')

    return train

train = load_train_data(filepath=FILE_PATH + 'train.txt', output_pickle=True, input_pickle=False)

# 随机选择的包
from numpy.random import choice
from numpy.random import seed
# 向下取整
from math import floor
import numpy as np
import pandas as pd
import pickle

# 将dict类型的train数据转为df类型,存到trainset.csv 和testset.csv
# test_size = 0.2 选取的测试集的比例
def train_test_divide(train=[], output_csv=False, input_data=False, test_size=0.2):
    print('begin divide train and test')
    if input_data is True:
        # 导入所有的数据集
        train = pickle.load(open(FILE_PATH + 'train.pickle', 'rb'))

    total_list = []
    # 存放test的数据
    testset = []
    print('begin divide test set')
    for id, item in train.items():
        # 设定随机数种子
        seed(SEED)
        # 从一个用户的所用评分中随机选择test_size比例的数据，作为测试集，不重复
        test_item_list = choice(list(item.keys()), size=floor(test_size * len(item.keys())), replace=False, )
        # test
        for test_item in test_item_list:
            testset.append([id, test_item, item[test_item]])
        # 所有的评分
        for item_id, score in item.items():
            total_list.append([id, item_id, score])
    # 将testset有dict转为df类型
    test_df = pd.DataFrame(data=testset, columns=['user', 'ID', 'score'])
    # 删去testset
    del testset
    
    print('begin divide traint set')
    # 选区total中有的但是测试集中没有的数据作为训练集
    # trainset = [i for i in total_list if i not in testset]
    # 将dict类型数据转为df类型
    total_df = pd.DataFrame(data=total_list, columns=['user', 'ID', 'score'])
    # 删去total list
    del total_list

    # 先扩展再去重，得到trian
    train_df = total_df.append(test_df)
    train_df['user'] = train_df['user'].astype(int)
    # 所有的train数据减去重复的就是所得剩下的train（此train包含着验证集，也就是说验证集还没有划分）
    train_df.drop_duplicates(subset=['user', 'ID', 'score'], keep=False, inplace=True)

    if output_csv is True:
        print('---------save as csv----------')
        total_df.set_index('user', inplace=True)
        total_df.to_csv(FILE_PATH + 'train.csv')
        del total_df
        train_df.set_index('user', inplace=True)
        train_df.to_csv(FILE_PATH + 'trainset.csv')
        del train_df
        test_df.set_index('user', inplace=True)
        test_df.to_csv(FILE_PATH + 'testset.csv')
        del test_df
    print('divide train and test end ')

train_test_divide(input_data=True, output_csv=True, test_size=0.2)

# 查看train trainset testset数据基本情况
total=pd.read_csv('train.csv')
print(total.describe())
del total
train = pd.read_csv('trainset.csv')
print(train.describe())
del train
test = pd.read_csv('testset.csv')
print(test.describe())
del test

import pandas as pd
# 加载item属性数据
# item dataframe columns=['ID', 'attribute1', 'attribute2']
def load_item(filepath, output_csv=False, input_csv=False):
    print("begin load data")

    if input_csv is True:
        item = pd.read_csv(FILE_PATH + 'item.csv')
    else:
        txt = np.loadtxt(filepath, dtype=str, delimiter='|')
        item = pd.DataFrame(data=txt, columns=['ID', 'attribute1', 'attribute2'])

        # 将None替换为0
        # item 属性中没有值为0的数据，所以这里可以用0来填充
        item.replace('None', 0, inplace=True)
        # 数值类型转化
        item['ID'] = item['ID'].astype(int)
        item['attribute1'] = item['attribute1'].astype(int)
        item['attribute2'] = item['attribute2'].astype(int)

        if output_csv is True:
            item.set_index('ID', inplace=True)
            item.to_csv(FILE_PATH + 'item.csv')
    print('load item data finish')
    del item

load_item(FILE_PATH + 'itemAttribute.txt', output_csv=True, input_csv=False)

item = pd.read_csv('item.csv')
print(item.describe())
del item

item = pd.read_csv('item.csv')
# 统计item中值为0的个数
temp_item = (item==0).astype(int)
del item
# print(temp_item.head())
atb1 = temp_item['attribute1'].sum()
atb2 = temp_item['attribute2'].sum()
del temp_item
print('attribute1 none: ', atb1, 'attribute2 none: ',atb2)

"""## 数据清洗
<hr>
"""

import pandas as pd
item = pd.read_csv('item.csv')
print(item.describe())

from tqdm.auto import tqdm
import pandas as pd 
# 数据清洗
def item_data_clearning(item, output_csv=False, already_cleaning=False):
    print('begin item data cleaning')
    print('------------------item data head----------------------')
    print(item.head())
    print('------------------item data describe----------------------')
    print(item.describe())
    print('------------------item data info ----------------------')
    print(item.info())
    if already_cleaning is True:
        return item
    else:
        # none值处理
        # attribute1的平均值
        attribute1_avg = item['attribute1'].mean()
        # 将0值即之间的none值替换为均值
        item['attribute1'].replace(0, attribute1_avg, inplace=True)
        # attribute2的平均值
        attribute2_avg = item['attribute2'].mean()
        # 将0值即之间的none值替换为均值
        item['attribute2'].replace(0, attribute2_avg, inplace=True)
        # print(item.head())

        # 物品属性缺失处理
        # ID max = 624960+1 从零开始计数的
        # ID rows = 507172
        # 缺失 624960+1-507172个，将这些值使用平均值进行填充，
        ID_max = item['ID'].max()
        print('--------------------ID_max------------------------')
        print(ID_max)
        # 实际上应该有的所有ID
        ID_full_list = set(range(ID_max))
        print('--------------------ID_full_list------------------------')
        # print(ID_full_list)
        # 数据集中给出的ID
        ID_list = set(item['ID'].tolist())
        print('--------------------ID_list------------------------')
        # print(ID_list)
        # 两者做差集求出缺失的ID
        ID_null = ID_full_list - ID_list
        print('--------------------ID_null------------------------')
        print(len(ID_null))
        # 将缺失的ID用均值进行填充
        df_list = []
        # 显示进度条
        with tqdm(total=len(ID_null), desc='ID null fill process') as bar:
            for id_null in ID_null:
                temp_dict = {'ID': id_null, 'attribute1': attribute1_avg, 'attribute2': attribute2_avg}
                df_list.append(temp_dict)
                bar.update(1)
        # 将list形式转化为df形式
        temp_df = pd.DataFrame(data=df_list, columns=['ID', 'attribute1', 'attribute2'])
        # 将新生成的df添加到item中
        item = item.append(temp_df, ignore_index=True)
        # 安装ID从小到大排序
        item.sort_values(by=['ID'], ascending=True, inplace=True)
        # 将ID设置为index
        item.set_index('ID', inplace=True)

        # 重复值处理
        # 无重复值
        print("--------------------- item duplication---------------------")
        print(item[item.duplicated()])

        # 输出为csv
        if output_csv is True:
            item.to_csv(FILE_PATH + 'item.csv')

    print('item data cleaning finish')
    return item

# item = item_data_clearning(item, True,False)
item = pd.read_csv('item.csv')
print(item.describe())

"""## 训练集，测试集划分
<hr>
"""

# 将dict类型的train数据转为df类型，并于item_plus 合并
# test_size = 0.2 选取的测试集的比例
def train_test_divide(train, output_csv=False, input_data=False, test_size=0.2):
    print('begin divide train and test')
    if input_data is True:
        # 导入item_plus
        item_plus = pd.read_csv(FILE_PATH + 'item_plus.csv')
        # item_plus = pd.read_csv(FILE_PATH + 'item_plus.csv', header=0, names=['Unnamed','user','ID_Power2','attribute2','attribute1','user_Power2','ID','user_Power2_multiply_ID','user_Power2_multiply_user','ID_multiply_ID_Power2'])
        # 导入所有的数据集
        train = pickle.load(open(FILE_PATH + 'train.pickle', 'rb'))
    # print(item_plus.head())

    # 去掉第一列
    # item_plus.drop(['Unnamed'], axis=1, inplace=True)
    # print(item_plus.head())

    total_list = []
    # 存放test的数据
    testset = []
    print('begin divide test set')
    for id, item in train.items():
        # 从一个用户的所用评分中随机选择test_size比例的数据，作为测试集，不重复
        test_item_list = choice(list(item.keys()), size=floor(test_size * len(item.keys())), replace=False)
        # test
        for test_item in test_item_list:
            testset.append([id, test_item, item[test_item]])
        # 所有的评分
        for item_id, score in item.items():
            total_list.append([id, item_id, score])
    # 将testset有dict转为df类型
    test_df = pd.DataFrame(data=testset, columns=['user', 'ID', 'score'])
    # 删去testset
    del testset

    print('begin divide traint set')
    # 选区total中有的但是测试集中没有的数据作为训练集
    # trainset = [i for i in total_list if i not in testset]
    # 将dict类型数据转为df类型
    total_df = pd.DataFrame(data=total_list, columns=['user', 'ID', 'score'])
    # 删去total list
    del total_list

    # 先扩展再去重，得到trian
    train_df = total_df.append(test_df)
    train_df['user'] = train_df['user'].astype(int)
    # 所有的train数据减去重复的就是所得剩下的train（此train包含着验证集，也就是说验证集还没有划分）
    train_df.drop_duplicates(subset=['user', 'ID', 'score'], keep=False, inplace=True)
    print('--------begin merge test and item plus ------')
    test_df_plus = pd.merge(test_df, item_plus, on=['user', 'ID'], how='left')

    print('-----------test_df_plus.describe()---------')
    print(test_df_plus.describe())

    if output_csv is True:
        print('---------save as csv----------')
        test_df.set_index('user', inplace=True)
        test_df.to_csv(FILE_PATH + 'testset.csv')
        test_df_plus.set_index('user', inplace=True)
        test_df_plus.to_csv(FILE_PATH + 'testset_plus.csv')

    del test_df_plus

    if output_csv is True:
        print('---------save as csv----------')
        total_df.set_index('user', inplace=True)
        total_df.to_csv(FILE_PATH + 'train.csv')
    del total_df

    print('--------begin merge train and item plus ------')
    train_df_plus = pd.merge(train_df, item_plus, on=['user', 'ID'], how='left')
    if output_csv is True:
        print('---------save as csv----------')

        train_df.set_index('user', inplace=True)
        train_df.to_csv(FILE_PATH + 'trainset.csv')
        train_df_plus.set_index('user', inplace=True)
        train_df_plus.to_csv(FILE_PATH + 'trainset_plus.csv')
    del train_df

    print('------------train_df_plus.describe()------------')
    print(train_df_plus.describe())
    del train_df_plus
    # 保存item plus
    # if output_csv is True:
        # print('---------save as csv----------')

        # item_plus.set_index('user', inplace=True)
        # item_plus.to_csv(FILE_PATH+'item_plus.csv')
    print('divide train and test end ')
    return 0

"""## 建立模型
<hr>

### 0模型
<hr>
"""

from sklearn.metrics import mean_squared_error
import pandas as pd
from math import sqrt
# 0模型
def zero_model(testset=[], input_csv=False):
    print('zero model begin')
    if input_csv is True:
        testset = pd.read_csv(FILE_PATH + 'testset.csv')
        trainset = pd.read_csv(FILE_PATH + 'trainset.csv')
    # 注意直接使用 test_predict1 = testset为浅拷贝
    # 使用testset.copy(deep=True) 时为深拷贝
    # 使用testset.copy(deep=False) 时为浅拷贝，相当于 test_predict1 = testset
    real_score = testset['score']
    test_predict = testset.copy(deep=True)
    del testset
    # 均值
    test_predict['pred_mean'] = trainset['score'].mean()
    test_predict['pred_50'] = 50
    # 中值
    test_predict['pred_median'] = trainset['score'].median()
    # 众值
    test_predict['pred_freq'] = trainset['score'].mode()[0]
    # 计算两种的rmse
    # zero_model_rmse1 38.223879691036416
    # 所得模型的rmse 必须比该值低才有效果
    zero_model_rmse1 = sqrt(mean_squared_error(real_score,test_predict['pred_mean']))
    # zero_model_rmse2 38.22696157454122
    zero_model_rmse2 = sqrt(mean_squared_error(real_score,test_predict['pred_50']))
    # 38.23329255713531
    zero_model_rmse3 = sqrt(mean_squared_error(real_score,test_predict['pred_median']))
    # 62.3791952651384
    zero_model_rmse4 = sqrt(mean_squared_error(real_score,test_predict['pred_freq']))
    print('mean rmse: ',zero_model_rmse1, '50 rmse: ', zero_model_rmse2, 'median rmse: ', zero_model_rmse3, 'freq rmse: ', zero_model_rmse4)

    print('zero model finish')
zero_model(input_csv=True)

"""### 基模型SVD
<hr>
"""

!pip install surprise

from sklearn import preprocessing
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from surprise import KNNBaseline,Dataset, Reader, BaselineOnly, accuracy, SVD, KNNBasic, CoClustering, KNNWithMeans, SVDpp,SlopeOne,NormalPredictor,AlgoBase,KNNWithZScore
from surprise.model_selection import cross_validate, KFold, PredefinedKFold, train_test_split, GridSearchCV
# 随机选择的包
from numpy.random import choice
# 向下取整
from math import floor
import time
import pandas as pd
import numpy as np
from surprise.dump import dump, load

"""##### 0-100"""

# 基模型
def svd(train=[], input_csv=False, output_csv=False):
    print('begin svd')
    begin = time.perf_counter()
    # 告诉文本阅读器，文本的格式是怎么样的
    reader = Reader(line_format='user item rating', sep=',', skip_lines=1,rating_scale=(0,100))
    # 从csv中加载数据
    if input_csv is True:
        # 指定文件所在路径
        file_path = os.path.expanduser('trainset.csv')
        # 加载数据
        train_cf = Dataset.load_from_file(file_path, reader=reader)
        # 如果训练所有数据取消掉下面的注释
        #---------------------------------------------------
        trainset = train_cf.build_full_trainset()
        #---------------------------------------------------
    else:
        # 从已有得df中加载数据
        train_cf = Dataset.load_from_df(train, reader=reader)

    # algo = SVD(n_epochs=40, lr_all=0.005, reg_all=0.01, n_factors=200)
    algo = SVD(n_epochs=5, lr_all=0.002, reg_all=0.2, n_factors=650)
    print('fit begin')
    fit_time_begin = time.perf_counter()

    algo.fit(trainset)

    fit_time_end = time.perf_counter()
    print('fit end')
    print('Running time: %s Seconds' % (fit_time_end - fit_time_begin))

    end = time.perf_counter()
    print('Running time: %s Seconds' % (end - begin))
    return algo

algo = svd(input_csv=True, output_csv=False)
  
test = pd.read_csv('testset.csv')
# 加载真实的score数据
test_score = test['score'].tolist()
# 遍历测试集进行预测
pred =[]
for row in test.itertuples():
  # 注意这里一定要 把 user ， item_id 转为str格式的
  pred.append(algo.predict(str(row[1]), str(row[2]), r_ui=row[3]).est)
del algo
# 四舍五入
pred_round = np.round(pred)
from sklearn.metrics import mean_squared_error
# 计算rmse
rmse = np.sqrt(mean_squared_error(test_score,pred_round))
print('rmse on test scale 0-100:', rmse)

"""##### 1-5"""

import time
all_begin = time.perf_counter()
# 基模型
def svd(train=[], input_csv=False):
    print('begin svd')
    begin = time.perf_counter()
    # 告诉文本阅读器，文本的格式是怎么样的
    reader = Reader(line_format='user item rating', sep=',', skip_lines=1,rating_scale=(1,5))
    # 从csv中加载数据
    if input_csv is True:
        # 指定文件所在路径
        file_path = os.path.expanduser('trainset_score[1,5]_2.csv')
        # 加载数据
        train_cf = Dataset.load_from_file(file_path, reader=reader)
        # 如果训练所有数据取消掉下面的注释
        #---------------------------------------------------
        trainset = train_cf.build_full_trainset()
        #---------------------------------------------------
    else:
        # 从已有得df中加载数据
        train_cf = Dataset.load_from_df(train, reader=reader)

    algo = SVD(n_epochs=150, lr_all=0.003, reg_all=0.01, n_factors=250)
    print('fit begin')
    fit_time_begin = time.perf_counter()

    algo.fit(trainset)

    fit_time_end = time.perf_counter()
    print('fit end')
    print('Running time: %s Seconds' % (fit_time_end - fit_time_begin))

    end = time.perf_counter()
    print('Running time: %s Seconds' % (end - begin))
    return algo

algo = svd(input_csv=True)
# 相当于swithch case 语句
def rescale1_5(score):
  switcher = {
      1: 10,
      2: 30,
      3: 50,
      4: 70,
      5: 90,
  }
  # 默认值为50
  return switcher.get(score,50)
  
total = pd.read_csv('train.csv')
test = pd.read_csv('testset_score[1,5]_2.csv')
# 合并两个df
temp_test = pd.merge(total, test, on=['user','ID'], how='inner')
# print(temp_test.head(10))
del total,test
# 加载真实的score数据
test_score = temp_test['score'].tolist()
# 遍历测试集进行预测
pred =[]
for row in temp_test.itertuples():
  # 注意这里一定要 把 user ， item_id 转为str格式的
  pred.append(algo.predict(str(row[1]), str(row[2]), r_ui=row[4]).est)
# 计算在1-5评分上的rmse
rmse_test = np.sqrt(mean_squared_error(temp_test['score[1,5]'].tolist(),pred))
print('rmse on test scale[1,5]:', rmse_test)
# 四舍五入
pred_round = np.round(pred)
# 从1-5转到原来的数据
pred_score = []
for p in pred_round:
  # 先转化为int
  pred_score.append(rescale1_5(int(p)))
from sklearn.metrics import mean_squared_error
# 计算rmse
rmse = np.sqrt(mean_squared_error(test_score,pred_score))
print('rmse on test scale 0-100:', rmse)
dump('svd-250.model', algo=algo, verbose=1)
del algo
all_end = time.perf_counter()
print('Running time: %s Seconds' % (all_end - all_begin))

import time
all_begin = time.perf_counter()
# 基模型
def svd(train=[], input_csv=False):
    print('begin svd')
    begin = time.perf_counter()
    # 告诉文本阅读器，文本的格式是怎么样的
    reader = Reader(line_format='user item rating', sep=',', skip_lines=1,rating_scale=(1,5))
    # 从csv中加载数据
    if input_csv is True:
        # 指定文件所在路径
        file_path = os.path.expanduser('trainset_score[1,5]_2.csv')
        # 加载数据
        train_cf = Dataset.load_from_file(file_path, reader=reader)
        # 如果训练所有数据取消掉下面的注释
        #---------------------------------------------------
        trainset = train_cf.build_full_trainset()
        #---------------------------------------------------
    else:
        # 从已有得df中加载数据
        train_cf = Dataset.load_from_df(train, reader=reader)

    algo = SVD(n_epochs=350, lr_all=0.003, reg_all=0.01, n_factors=250)
    print('fit begin')
    fit_time_begin = time.perf_counter()

    algo.fit(trainset)

    fit_time_end = time.perf_counter()
    print('fit end')
    print('Running time: %s Seconds' % (fit_time_end - fit_time_begin))

    end = time.perf_counter()
    print('Running time: %s Seconds' % (end - begin))
    return algo

algo = svd(input_csv=True)
# 相当于swithch case 语句
def rescale1_5(score):
  switcher = {
      1: 10,
      2: 30,
      3: 50,
      4: 70,
      5: 90,
  }
  # 默认值为50
  return switcher.get(score,50)
  
total = pd.read_csv('train.csv')
test = pd.read_csv('testset_score[1,5]_2.csv')
# 合并两个df
temp_test = pd.merge(total, test, on=['user','ID'], how='inner')
# print(temp_test.head(10))
del total,test
# 加载真实的score数据
test_score = temp_test['score'].tolist()
# 遍历测试集进行预测
pred =[]
for row in temp_test.itertuples():
  # 注意这里一定要 把 user ， item_id 转为str格式的
  pred.append(algo.predict(str(row[1]), str(row[2]), r_ui=row[4]).est)
# 计算在1-5评分上的rmse
rmse_test = np.sqrt(mean_squared_error(temp_test['score[1,5]'].tolist(),pred))
print('rmse on test scale[1,5]:', rmse_test)
# 四舍五入
pred_round = np.round(pred)
# 从1-5转到原来的数据
pred_score = []
for p in pred_round:
  # 先转化为int
  pred_score.append(rescale1_5(int(p)))
from sklearn.metrics import mean_squared_error
# 计算rmse
rmse = np.sqrt(mean_squared_error(test_score,pred_score))
print('rmse on test scale 0-100:', rmse)
dump('svd-350.model', algo=algo, verbose=1)
del algo
all_end = time.perf_counter()
print('Running time: %s Seconds' % (all_end - all_begin))

"""##### user cf 1-5"""

# 基模型
def user_cf(train=[], input_csv=False):
    print('begin user cf')
    begin = time.perf_counter()
    # 告诉文本阅读器，文本的格式是怎么样的
    reader = Reader(line_format='user item rating', sep=',', skip_lines=1,rating_scale=(1,5))
    # 从csv中加载数据
    if input_csv is True:
        # 指定文件所在路径
        file_path = os.path.expanduser('trainset_score[1,5]_2.csv')
        # 加载数据
        train_cf = Dataset.load_from_file(file_path, reader=reader)
        # 如果训练所有数据取消掉下面的注释
        #---------------------------------------------------
        trainset = train_cf.build_full_trainset()
        #---------------------------------------------------
    else:
        # 从已有得df中加载数据
        train_cf = Dataset.load_from_df(train, reader=reader)

    sim_options = {'name': 'pearson_baseline',
                  'shrinkage': 100  #  shrinkage
               }
    algo = KNNWithMeans(k=40,sim_options=sim_options) # 1.52

    # 如果训练所有数据取消掉下面的注释
    #---------------------------------------------------
    # algo = SVD(n_epochs=150, lr_all=0.003, reg_all=0.01, n_factors=250)
    print('fit begin')
    fit_time_begin = time.perf_counter()

    algo.fit(trainset)

    fit_time_end = time.perf_counter()
    print('fit end')
    print('Running time: %s Seconds' % (fit_time_end - fit_time_begin))

    end = time.perf_counter()
    print('Running time: %s Seconds' % (end - begin))
    return algo

algo = user_cf(input_csv=True)
# 相当于swithch case 语句
def rescale1_5(score):
  switcher = {
      1: 10,
      2: 30,
      3: 50,
      4: 70,
      5: 90,
  }
  # 默认值为50
  return switcher.get(score,50)
  
total = pd.read_csv('train.csv')
test = pd.read_csv('testset_score[1,5]_2.csv')
# 合并两个df
temp_test = pd.merge(total, test, on=['user','ID'], how='inner')
# print(temp_test.head(10))
del total,test
# 加载真实的score数据
test_score = temp_test['score'].tolist()
# 遍历测试集进行预测
pred =[]
for row in temp_test.itertuples():
  # 注意这里一定要 把 user ， item_id 转为str格式的
  pred.append(algo.predict(str(row[1]), str(row[2]), r_ui=row[4]).est)
# 计算在1-5评分上的rmse
rmse_test = np.sqrt(mean_squared_error(temp_test['score[1,5]'].tolist(),pred))
print('rmse on test scale[1,5]:', rmse_test)
# 四舍五入
pred_round = np.round(pred)
# 从1-5转到原来的数据
pred_score = []
for p in pred_round:
  # 先转化为int
  pred_score.append(rescale1_5(int(p)))
from sklearn.metrics import mean_squared_error
# 计算rmse
rmse = np.sqrt(mean_squared_error(test_score,pred_score))
print('rmse on test scale 0-100:', rmse)

"""#### final"""

# 基模型
def svd2(train=[], input_csv=False):
    print('begin svd')
    begin = time.perf_counter()
    # 告诉文本阅读器，文本的格式是怎么样的
    reader = Reader(line_format='user item rating', sep=',', skip_lines=1,rating_scale=(1,5))
    # 从csv中加载数据
    if input_csv is True:
        # 指定文件所在路径
        file_path = os.path.expanduser('train_score[1,5].csv')
        # 加载数据
        train_cf = Dataset.load_from_file(file_path, reader=reader)
        # 如果训练所有数据取消掉下面的注释
        #---------------------------------------------------
        trainset = train_cf.build_full_trainset()
        #---------------------------------------------------
    else:
        # 从已有得df中加载数据
        train_cf = Dataset.load_from_df(train, reader=reader)

    # 如果训练所有数据取消掉下面的注释
    #---------------------------------------------------
    algo = SVD(n_epochs=350, lr_all=0.003, reg_all=0.01, n_factors=250)
    print('fit begin')
    fit_time_begin = time.perf_counter()

    algo.fit(trainset)

    fit_time_end = time.perf_counter()
    print('fit end')
    print('Running time: %s Seconds' % (fit_time_end - fit_time_begin))

    end = time.perf_counter()
    print('Running time: %s Seconds' % (end - begin))
    return algo

algo2 = svd2(input_csv=True)
# 保存模型
dump('svd-350-2.model', algo=algo2, verbose=1)

"""## 特征工程部分
<hr>

### item属性的特征构建，特征选择与特征提取
<hr>
"""

from pycaret.regression import *
reg1 = setup(data = train, target='score',feature_interaction=True, feature_ratio=True, polynomial_features=True, train_size=0.8,feature_selection=True, feature_selection_threshold=0.6)
  # 所有的数据
total = reg1[0]

"""### 对score做对数变化
<hr>
"""

import pandas as pd
import numpy as np
# 训练集scale log
train = pd.read_csv('trainset.csv')
# 第三步：对收入的数据进行log变化
train['new_score'] = np.log(train['score'].values+1)
# 去掉score这一列
train.drop('score',inplace=True,axis=1)
train.set_index('user', inplace=True)
train.to_csv('new_score_trainset.csv')

# 训练集scale log
test = pd.read_csv('testset.csv')
# 第三步：对收入的数据进行log变化
test['new_score'] = np.log(test['score'].values+1)
# 去掉score这一列
test.drop('score',inplace=True,axis=1)
test.set_index('user', inplace=True)
test.to_csv('new_score_testset.csv')

# 绘制score的分布图
train['score'].plot.hist(range=(0,100))

# 第三步：对收入的数据进行log变化
train['new_score'] = np.log(train['score'].values+1)
print(train[['score', 'new_score']].head())

train.drop(['score'], axis=1,inplace=True)

# 绘制new_score的分布图
train['new_score'].plot.hist(range=(0,100))

"""### 1-100 to 1-5"""

def scale5(score):
  if 0 <= score < 20:
    return 1
  elif 20 <= score < 40:
    return 2
  elif 40 <= score < 60:
    return 3
  elif 60 <= score < 80:
    return 4
  elif 80 <= score <=100:
    return 5 

import pandas as pd
import numpy as np
# 训练集scale 1- 10
train = pd.read_csv('trainset.csv')
train['score[1,5]'] = train['score'].apply(scale5)
print(train.describe())
# 去掉score这一列
train.drop('score',inplace=True,axis=1)
train.set_index('user', inplace=True)
train.to_csv('trainset_score[1,5].csv')
del train

# 测试集scale 1- 5
test = pd.read_csv('testset.csv')
test['score[1,5]'] = test['score'].apply(scale5)
test.drop('score',inplace=True,axis=1)
test.set_index('user', inplace=True)
test.to_csv('testset_score[1,5].csv')
del test

# total train
def scale5(score):
  if 0 <= score < 20:
    return 1
  elif 20 <= score < 40:
    return 2
  elif 40 <= score < 60:
    return 3
  elif 60 <= score < 80:
    return 4
  elif 80 <= score <=100:
    return 5 

import pandas as pd
import numpy as np
# 训练集scale 1- 10
train = pd.read_csv('train.csv')
train['score[1,5]'] = train['score'].apply(scale5)
print(train.describe())
# 去掉score这一列
train.drop('score',inplace=True,axis=1)
train.set_index('user', inplace=True)
train.to_csv('train_score[1,5].csv')
del train

"""### 1-100 to 1-10"""

def scale10(score):
  if 0 <= score < 10:
    return 1
  elif 10 <= score < 20:
    return 2
  elif 20 <= score < 30:
    return 3
  elif 30 <= score < 40:
    return 4
  elif 40 <= score < 50:
    return 5 
  elif 50 <= score < 60:
    return 6
  elif 60 <= score < 70:
    return 7
  elif 70 <= score < 80:
    return 8
  elif 80 <= score < 90:
    return 9
  elif 90 <= score <= 100:
    return 10

import pandas as pd
import numpy as np
train = pd.read_csv('trainset.csv')
train['score[1,10]'] = train['score'].apply(scale10)
print(train.describe())
# 去掉score这一列
train.drop('score',inplace=True,axis=1)
train.set_index('user', inplace=True)
train.to_csv('trainset_score[1,10].csv')

test = pd.read_csv('testset.csv')
test['score[1,10]'] = test['score'].apply(scale10)
print(test.describe())
# 去掉score这一列
test.drop('score',inplace=True,axis=1)
test.set_index('user', inplace=True)
test.to_csv('testset_score[1,10].csv')

"""### 0-100 to 0-10"""

def scale0_10(score):
  if 0 < score < 10:
    return 1
  elif 10 <= score < 20:
    return 2
  elif 20 <= score < 30:
    return 3
  elif 30 <= score < 40:
    return 4
  elif 40 <= score < 50:
    return 5 
  elif 50 <= score < 60:
    return 6
  elif 60 <= score < 70:
    return 7
  elif 70 <= score < 80:
    return 8
  elif 80 <= score < 90:
    return 9
  elif 90 <= score <= 100:
    return 10
  elif score == 0:
    return 0

import pandas as pd
import numpy as np
train = pd.read_csv('trainset.csv')
train['score[0,10]'] = train['score'].apply(scale10)
print(train.describe())
# 去掉score这一列
train.drop('score',inplace=True,axis=1)
train.set_index('user', inplace=True)
train.to_csv('trainset_score[0,10].csv')

test = pd.read_csv('testset.csv')
test['score[0,10]'] = test['score'].apply(scale10)
print(test.describe())
# 去掉score这一列
test.drop('score',inplace=True,axis=1)
test.set_index('user', inplace=True)
test.to_csv('testset_score[0,10].csv')

"""## EDA（探索性数据分析）
<hr>
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

train = pd.read_csv('trainset.csv')

item_max = train['ID'].max()
user_max = train['user'].max()
length = len(train['user'])
# 计算矩阵的稀疏程度
sparsity = 1-length/(item_max*user_max)
print(sparsity)

train = pd.read_csv('trainset_score[1,5].csv')
score_num = train.groupby('score[1,5]').size()
score_num = score_num.reset_index()
score_num.sort_values(0 ,inplace=True, ascending=False)
score_num.set_index('score[1,5]', inplace=True)
print(score_num)

"""## ML
<hr>
"""

# 安装pycaret包
!pip install pycaret

# 当前的路径
import os
os.chdir("/content/drive/My Drive/recommention")

import pandas as pd
import numpy as np
new_train = pd.read_csv('new_score_trainset_plus.csv')
new_train['attribute1/attribute2'] = new_train['attribute1']/new_train['attribute2']
new_train['attribute1+attribute2'] = new_train['attribute1']+new_train['attribute2']
new_train['attribute1_power2'] = np.power(new_train['attribute1'],2)
new_train['attribute2_power2'] = np.power(new_train['attribute2'],2)
new_train['attribute1_power2+attribute2'] = np.power(new_train['attribute1'],2) + new_train['attribute2']
new_train['attribute2_power2+attribute1'] = np.power(new_train['attribute2'],2) + new_train['attribute1']

# import regression module
from pycaret.regression import *
# intialize the setup
reg1 = setup(data = new_train, target='new_score', train_size=0.8)

# lightgbm
lightgbm = create_model('lightgbm', fold=2) 
# xgboost
xgboost = create_model('xgboost',fold=2)
# K Neighbors Regressor
knn = create_model('knn',fold=2)
# Ridge Regression
ridge = create_model('ridge',fold=2)
# creating multiple layer stacking from specific models
stacknet = create_stacknet([[lightgbm, knn], [ridge, xgboost]], fold=2)
#save trained model
save_model(stacknet, 'ml')

new_test = pd.read_csv('new_score_testset_plus.csv')
new_test['attribute1/attribute2'] = new_test['attribute1']/new_test['attribute2']
new_test['attribute1+attribute2'] = new_test['attribute1']+new_test['attribute2']
new_test['attribute1_power2'] = np.power(new_test['attribute1'],2)
new_test['attribute2_power2'] = np.power(new_test['attribute2'],2)
new_test['attribute1_power2+attribute2'] = np.power(new_test['attribute1'],2) + new_test['attribute2']
new_test['attribute2_power2+attribute1'] = np.power(new_test['attribute2'],2) + new_test['attribute1']
test_score = new_test['new_score'].tolist()
new_test.drop('new_score',axis=1, inplace=True)

print(new_test.head())
# generate predictions on unseen data
pred = predict_model(stacknet, data = new_test)
# 从log转到原来的数据
pred_score = np.ceil(np.exp(pred['Label'].tolist())+1)

temp_test = pd.read_csv('testset.csv')
test_score = temp_test['score'].tolist()
del temp_test

from sklearn.metrics import mean_squared_error
# 计算rmse
rmse = np.sqrt(mean_squared_error(test_score,pred_score))
rmse

"""## 生成test 结果
<hr>
"""

# 加载test文件
def load_test_data(filepath, output_csv=False):
    print('begin load test data ')
    # 打开文件
    with open(filepath, 'r') as f:
        test = []
        while True:
            line = f.readline()
            if not line or line == '\n':
                break

            id, item_num = line.split('|')
            # 类型转化
            id = int(id)
            item_num = int(item_num)

            # 遍历之后的内容
            for i in range(item_num):
                line = f.readline()
                item_id = line
                # 数据类型转化
                item_id = int(item_id)
                # 放入test中
                test.append([id, item_id, 0])
    # 转为df类型
    test = pd.DataFrame(data=test, columns=['user', 'ID', 'score'])
    test.set_index('user', inplace=True)

    if output_csv is True:
        test.to_csv(FILE_PATH+'test.csv')
    print('load test data finish')
    return test

from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
from surprise.dump import dump, load
temp_pred,algo= load('svd-350-2.model')
del temp_pred
# 相当于swithch case 语句
def rescale1_5(score):
  switcher = {
      1: 10,
      2: 30,
      3: 50,
      4: 70,
      5: 90,
  }
  # 默认值为50
  return switcher.get(score,50)
  
test = pd.read_csv('test.csv')
# 遍历测试集进行预测
pred =[]
for row in test.itertuples():
  # 注意这里一定要 把 user ， item_id 转为str格式的
  pred.append(algo.predict(str(row[1]), str(row[2]), r_ui=row[3]).est)
del algo
# 四舍五入
pred_round = np.round(pred)
# 从1-5转到原来的数据
pred_score = []
for p in pred_round:
  # 先转化为int
  pred_score.append(rescale1_5(int(p)))
test['pred'] = pred_score
test.drop('score', axis=1, inplace=True)
test.set_index('user', inplace=True)
test.to_csv('submit2.csv')
print(test.head(10))
del test

test = pd.read_csv('submit2.csv')
# 写入text
with open("submit2.txt","w") as f:
  temp_user = 1
  for row in test.itertuples():
    if temp_user != row[1]:
      f.write(str(row[1])+'|6\n')
      temp_user = row[1]
    f.write(str(row[2])+ " "+ str(row[3])+"\n")

temp_test = test.groupby('user').size()
# 发现测试集中每个用户只对6个item评分
temp_test.describe()

# 查看写入后的数据集
with open("submit2.txt","r") as f:
  result = f.read()
  print(result)